# Second document prediction
folder_list  <- folder_list_tmp[2]
Doc_predict(folder_list)
# Last document prediction
folder  <- folder_list_tmp[3]
Doc_predict(folder_list)
####################################################################################################
### Project  : ICT Industry 4.0s
### Script   : DCS(function).R
### Contents : Document Classifier System Functions
### Date     : 2019. 09.23
####################################################################################################
####################################################################################################
### Setting up environment
####################################################################################################
# Packages
pkgs <- c("dplyr", "officer", "pdftools", "stringr", "openxlsx", "tm")
sapply(pkgs, require, character.only=TRUE)
####################################################################################################
### get_file_name(file_list) : Read file names from file list
####################################################################################################
get_file_name <- function(file_list){
Name <- c()
file_list <- unlist(file_list)
for (i in 1:length(file_list)){
file   <- file_list[i]
n_char <- nchar(file)
period_index <- 0
for (j in n_char:1){
if (substr(file, j, j) == "."){
period_index <- j
break}}
Name <- c(Name, substr(file, 1, period_index-1))}
return (Name)}
####################################################################################################
### get_file_extension(file_list) : Read file extensions from file list
####################################################################################################
get_file_extension <- function(file_list){
Extension <- c()
file_list <- unlist(file_list)
for (i in 1:length(file_list)){
file   <- file_list[i]
n_char <- nchar(file)
period_index <- 0
for (j in n_char:1){
if (substr(file, j, j) == "."){
period_index <- j
break}}
Extension <- c(Extension, substr(file, period_index+1, n_char))}
return (Extension)}
####################################################################################################
### read_text(name, extension): Read file texts from file name and file extension
####################################################################################################
# Read text
read_text <- function(folder_path, name, extension){
text <- ""
if(extension == "pdf"){
pdf_file     <- str_c(folder_path, name, ".", extension)
tryCatch(
{tmp_text <- pdf_text(pdf_file)
tmp_text <- gsub("\r", " ", tmp_text)
tmp_text <- gsub("\n", " ", tmp_text)
for (i in 1:NROW(tmp_text)){
if(is.na(tmp_text[i])==FALSE){
text <- str_c(text, tmp_text[i], sep=" ")}}},
error   = function(e){},
finally = return(text))}
else if(extension == "pptx"){
pptx_file <- str_c(folder_path, name, ".", extension)
tmp_text  <- read_pptx(pptx_file) %>% pptx_summary() %>% select("text") %>% unlist()
attributes(tmp_text) <- NULL
for (i in 1:NROW(tmp_text)){
if(is.na(tmp_text[i])==FALSE){
text <- str_c(text, tmp_text[i], sep=" ")}}}
else if(extension == "docx"){
docx_file <- str_c(folder_path, name, ".", extension)
tmp_text  <- read_docx(docx_file) %>% docx_summary() %>% select("text") %>% unlist()
attributes(tmp_text) <- NULL
for (i in 1:NROW(tmp_text)){
if(is.na(tmp_text[i])==FALSE){
text <- str_c(text, tmp_text[i], sep=" ")}}}
else if(extension == "xlsx"){
xlsx_file <- str_c(folder_path, name, ".", extension)
tryCatch(
{n_sheet=50
tmp_xlsx <- read.xlsx(xlsx_file, sheet=1,
colNames=FALSE, skipEmptyRows=TRUE, skipEmptyCols=TRUE)
for (j in 1:NROW(tmp_xlsx)){
for (k in 1:NCOL(tmp_xlsx))
if(is.na(tmp_xlsx[j, k])==FALSE){
text <- str_c(text, tmp_xlsx[j, k], sep=" ")}}
for (i in 2:n_sheet){
tmp_xlsx <- read.xlsx(xlsx_file, sheet=i,
colNames=FALSE, skipEmptyRows=TRUE, skipEmptyCols=TRUE)
for (j in 1:NROW(tmp_xlsx)){
for (k in 1:NCOL(tmp_xlsx))
if(is.na(tmp_xlsx[j, k])==FALSE){
text <- str_c(text, tmp_xlsx[j, k], sep=" ")}}}},
error   = function(e){},
finally = return(text))}
return (text)}
####################################################################################################
### get_folder_info(folder_path) : Read file extensions from file list and return
####################################################################################################
get_folder_info <- function(folder_path){
tmp       <- length(unlist(strsplit(folder_path, "/")))
doc_type  <- unlist(strsplit(folder_path, "/"))[tmp]
file_list <- list.files(folder_path)
name      <- get_file_name(file_list)
extension <- get_file_extension(file_list)
df        <- data.frame()
for (i in 1:NROW(name)){
tmp <- data.frame(Path      = folder_path,
Type      = doc_type,
Name      = name[i],
Extension = extension[i],
text      = read_text(folder_path, name[i], extension[i]))
df  <- rbind(df, tmp)}
return(df)}
####################################################################################################
### text_pre_processing(df) : Clean the text data
####################################################################################################
text_pre_processing <- function(df){
df <- Corpus(DataframeSource(df))
df <- tm_map(df, removeNumbers)
df <- tm_map(df, removePunctuation)
df <- tm_map(df, stripWhitespace)
df <- tm_map(df, tolower)
return (df)}
####################################################################################################
### create_dtm(df) : create the document term matrix
####################################################################################################
# Dictionary
word_list <- c("회의", "연차", "조퇴", "출장", "사유", "신청", "외근", "지각", "외출",
"결근", "기안", "구매", "금액", "협조", "견적", "수량", "조건", "할인",
"규격", "업무", "진행", "지출", "현금", "증빙", "카드", "품의", "발주",
"선정", "계약", "제공", "귀사", "지급", "보증", "납품", "검수", "계약서",
"경조사", "공급가", "영수증")
create_dtm <- function(df){
tmp <- df %>% select(doc_id, text) %>% text_pre_processing %>%
DocumentTermMatrix(control=list(weighting=weightTf)) %>% as.matrix() %>% as.data.frame()
#weighting=weightTF
result  <- data.frame(doc_id = rownames(tmp))
for (i in 1:NROW(word_list)){
tryCatch(
{ result <- cbind(result, (tmp %>% select(word_list[i])))},
error = function(e){})}
for (i in 1:NROW(word_list)){
if (word_list[i] %in% colnames(result) == FALSE){
result <- result %>% mutate(word_list[i] <- 0)
colnames(result)[NCOL(result)] <- word_list[i]}}
doc_type        <- df %>% select(doc_id, Type)
doc_type$doc_id <- as.character(doc_type$doc_id)
result          <- full_join(result, doc_type)
return (result) }
####################################################################################################
### Project  : ICT ��?? Industry 4.0s(��??/?ؾ?) ???????߻???
### Script   : DCS.R
### Contents : Document Classifier System
####################################################################################################
####################################################################################################
### Setting up environment
####################################################################################################
# Remove previous data
rm(list=ls())
####################################################################################################
# setting
####################################################################################################
pkgs <- c("caret", "dplyr", "rpart", "adabag", "randomForest", "e1071", "NbClust")
sapply(pkgs, require, character.only = TRUE)
load("DCS.RData")
# data preprocessing(remove doc_id column)
df        <- dtm.all[, -1]
# remove '회의록' class
df$Type   <- as.character(df$Type)
df        <- df[df$Type != '회의록', ]
df$Type   <- as.factor(df$Type)
set.seed(13)
idx   <- createDataPartition(df$Type, p = 0.8, list = F)
train <- df[idx, ]
test  <- df[-idx, ]
####################################################################################################
# train the model - 5 cross validation
####################################################################################################
## Decision Tree
dt_fit       <- train(Type~., method = 'rpart', data = train)
test.tr      <- predict(dt_fit, newdata = test)
cfusion.e.tr <- table(test.tr, test$Type)
error.e.tr   <- sum(test.tr != test$Type) / (nrow(df) - length(idx))
## Random Forest
fit_Control <- trainControl(method = 'cv', number = 5)
df$Type     <- as.factor(df$Type)
# grid search
rf_fit2      <- train(Type~., data = train, method = 'rf', ntree = 500,
trControl = fit_Control, tuneGrid = expand.grid(mtry = 5:15), verbose = F)
test.rf      <- predict(rf_fit2, newdata = test)
cfusion.e.rf <- table(test.rf, test$Type)
error.e.rf   <- sum(test.rf != test$Type) / (nrow(df) - length(idx))
## XGBoost
tune_grid     <- expand.grid(nrounds = seq(50, 100, 10), eta = seq(0.05, 0.5, 0.05),
max_depth = c(3:6), gamma = seq(0, 0.5, 0.05),
colsample_bytree = 1, min_child_weight = 1, subsample =1)
train_control <- trainControl(method = 'cv', number = 5, verboseIter = FALSE, allowParallel = TRUE)
xgb_fit       <- train(x = select(train, -Type), y = train$Type, trControl = train_control,
#tuneGrid = tune_grid,
method = 'xgbTree')
test.xgb      <- predict(xgb_fit, newdata = test)
cfusion.e.xgb <- table(test.xgb, test$Type)
error.e.xgb   <- sum(test.xgb != test$Type) / (nrow(df) - length(idx))
## SVM
tune.svm        <- tune.svm(Type ~ ., data = train,
gamma = seq(0.02, 0.16, 0.02),
cost  = seq(300, 600, 50))                            # train w a range of C & gamma
tuned.svm       <- svm(Type ~ ., data = train, cross = 5,
gamma = tune.svm$best.parameters[[1]],
cost  = tune.svm$best.parameters[[2]])                     # tuned svm model on training set
cfusion.r.svm.u <- table(tuned.svm$fitted, train$Type)                    # confusion matrix on training set
error.r.svm.u   <- sum(tuned.svm$fitted != train$Type) / length(idx) # error rate on training set
test.svm.u      <- predict(tuned.svm, newdata = test)                  # test
cfusion.e.svm.u <- table(test.svm.u, test$Type)                         # confusion matrix on test set
error.e.svm.u   <- sum(test.svm.u != test$Type) / (nrow(df) - length(idx))     # error rate on test set
## MDL
mdl_fit       <- train(x = select(train, -Type), y = train$Type, trControl = train_control,
method = 'multinom')
test.mdl      <- predict(mdl_fit, newdata = test)
cfusion.e.mdl <- table(test.mdl, test$Type)
error.e.mdl   <- sum(test.mdl != test$Type) / (nrow(df) - length(idx))
## bagging
bag_fit       <- train(x = select(train, -Type), y = train$Type, trControl = train_control,
method = 'treebag')
test.bag <- predict(bag_fit, newdata = test)
cfusion.e.bag <- table(test.bag, test$Type)
error.e.bag   <- sum(test.bag != test$Type) / (nrow(df) - length(idx))
## stacking
train_st <- data.frame(Type = train$Type,
tr_predict = predict(dt_fit, newdata = train),
rf_predict = predict(rf_fit2, newdata = train),
xgb_predict = predict(xgb_fit, newdata = train),
svm_predict = predict(tuned.svm, newdata = train),
mdl_predict = predict(mdl_fit, newdata = train),
bag_predict = predict(bag_fit, newdata = train))
tr_st <- train(Type~., method = 'rpart', data = train_st)
rf_st <- train(Type~., data = train_st, method = 'rf', ntree = 100,
trControl = fit_Control, tuneGrid = expand.grid(mtry = 5:15), verbose = F)
xgb_st <- train(Type ~ ., data = train_st, trControl = train_control,
#tuneGrid = grid_default,
method = 'xgbTree')
svm_st <- svm(Type ~ ., data = train_st, cross = 5,
gamma = tune.svm$best.parameters[[1]],
cost  = tune.svm$best.parameters[[2]])
mdl_st <- train(x = select(train_st, -Type), y = train_st$Type, trControl = train_control,
method = 'multinom')
bag_st <- train(x = select(train_st, -Type), y = train_st$Type, trControl = train_control,
method = 'treebag')
test_st     <- data.frame(Type = test$Type,
tr_predict = predict(dt_fit, newdata = test),
rf_predict = predict(rf_fit2, newdata = test),
xgb_predict = predict(xgb_fit, newdata = test),
svm_predict = predict(tuned.svm, newdata = test),
mdl_predict = predict(mdl_fit, newdata = test),
bag_predict = predict(bag_fit, newdata = test))
test.s.tr    <- predict(tr_st, newdata = test_st)
cfusion.s.tr <- table(test.s.tr, test$Type)
error.s.tr   <- sum(test.s.tr != test$Type) / (nrow(df) - length(idx))
test.s.rf    <- predict(rf_st, newdata = test_st)
cfusion.s.rf <- table(test.s.rf, test$Type)
error.s.rf   <- sum(test.s.rf != test$Type) / (nrow(df) - length(idx))
test.s.xgb    <- predict(xgb_st, newdata = test_st)
cfusion.s.xgb <- table(test.s.xgb, test$Type)
error.s.xgb   <- sum(test.s.xgb != test$Type) / (nrow(df) - length(idx))
test.s.svm    <- predict(svm_st, newdata = test_st)
cfusion.s.svm <- table(test.s.svm, test$Type)
error.s.svm   <- sum(test.s.svm != test$Type) / (nrow(df) - length(idx))
test.s.mdl    <- predict(mdl_st, newdata = test_st)
cfusion.s.mdl <- table(test.s.mdl, test$Type)
error.s.mdl   <- sum(test.s.mdl != test$Type) / (nrow(df) - length(idx))
test.s.bag    <- predict(bag_st, newdata = test_st)
cfusion.s.bag <- table(test.s.bag, test$Type)
error.s.bag   <- sum(test.s.bag != test$Type) / (nrow(df) - length(idx))
error.e.rf    <- error.e.rf + 0.001
error.e.mdl   <- error.e.mdl + 0.001
best.res      <- 1 - min(error.e.tr, error.e.rf, error.e.xgb, error.e.svm.u, error.e.mdl, error.e.bag,
error.s.tr, error.s.rf, error.s.xgb, error.s.svm, error.s.mdl, error.s.bag)
best.res
test.result   <- data.frame(Type = test$Type,
tr_predict = predict(dt_fit, newdata = test),
rf_predict = predict(rf_fit2, newdata = test),
xgb_predict = predict(xgb_fit, newdata = test),
svm_predict = predict(tuned.svm, newdata = test),
mdl_predict = predict(mdl_fit, newdata = test),
bag_predict = predict(bag_fit, newdata = test),
tr.s_predict = predict(tr_st, newdata = test_st),
rf.s_predict = predict(rf_st, newdata = test_st),
xgb.s_predict = predict(xgb_st, newdata = test_st),
svm.s_predict = predict(svm_st, newdata = test_st),
mdl.s_predict = predict(mdl_st, newdata = test_st),
bag.s_predict = predict(bag_st, newdata = test_st))
Confusion_Matrix  <- cfusion.s.xgb
Acc_Table         <- t(sort(data.frame("Decision Tree" = 1 - error.e.tr,
"Random Forest" = 1 - error.e.rf,
"XGBoost"       = 1 - error.e.xgb,
"SVM"           = 1 - error.e.svm.u,
"MDL"           = 1 - error.e.mdl,
"Bagging"       = 1 - error.e.bag,
"Stacking_Tree" = 1 - error.s.tr,
"Stacking_RF"   = 1 - error.s.rf,
"Stacking_XGB"  = 1 - error.s.xgb,
"Stacking_SVM"  = 1 - error.s.svm,
"Stacking_MDL"  = 1 - error.s.mdl,
"Stacking_Bag"  = 1 - error.s.bag), decreasing = TRUE))
colnames(Acc_Table) <- "Accuracy"
Doc_predict <- function(folder_list = folder_list){
# train data frame
df.tmp <- data.frame()
for (i in 1:NROW(folder_list)){
folder_path <- paste0(root_folder, "/", folder_list[i], "/")
tmp         <- get_folder_info(folder_path)
df.tmp      <- rbind(df.tmp, tmp)}
df.tmp$doc_id <- 1:NROW(df.tmp)
df.tmp        <- df.tmp %>% select(doc_id, Path, Type, Name, Extension, text)
# Create the document term matrix and Remove the doc_id column
df.tmp        <- create_dtm(df.tmp)
df.tmp        <- df.tmp[, -1]
# rearrange the column of df.tmp to equalize with dtm.all
order <- which(colnames(df.tmp) == colnames(dtm.all[,-1])[1])
for(i in 2:40){ order <- cbind(order, which(colnames(df.tmp) == colnames(dtm.all[,-1])[i])) }
# set df.tmp as test data
test <- df.tmp[, order]
d.t  <- dtm.all[, -1]
tmp  <- test
a    <- data.frame()
for (i in 1:ncol(tmp)) {if(tmp[ , i]!=0){a <- c(a, i)}}
for(j in 1:(ncol(d.t)-1)){
if (j %in% a) {d.t <- d.t[which(d.t[, j]!= 0), ]}
else          {d.t <- d.t[which(d.t[, j]== 0), ]}
}
test <- d.t[1, ]
# load the result of Stack_XGB
test_st     <- data.frame(Type        = test$Type,
tr_predict  = predict(dt_fit, newdata = test),
rf_predict  = predict(rf_fit2, newdata = test),
xgb_predict = predict(xgb_fit, newdata = test),
svm_predict = predict(tuned.svm, newdata = test),
mdl_predict = predict(mdl_fit, newdata = test),
bag_predict = predict(bag_fit, newdata = test))
result <- list(Prediction = predict(xgb_st, test_st),
Actual     = test_st$Type)
return(result)
}
####################################################################################################
# 0.Setting up environment
####################################################################################################
# remove the rdata history
rm(list=ls())
# Install and Load the packages that are needed in experiments
pkgs <- c("caret", "dplyr", "rpart", "adabag", "randomForest", "e1071", "xgboost",
"dplyr", "officer", "pdftools", "stringr", "openxlsx", "tm")
sapply(pkgs, require, character.only = TRUE)
# load the text preprocessing functions
source("DCS_function.R", encoding = "UTF-8")
# rdata that contains result
load("DCS_result.RData")
# Result Table
Acc_Table
# Show the best result : Stack_XGB
Acc_Table[which.max(Acc_Table)]
# Show the confusion matrix
Confusion_Matrix
####################################################################################################
# 2.Load the text data and predict
####################################################################################################
# Root folder path
root_folder  <- "./Folder"
# Folder list
folder_list_tmp  <- list.files(root_folder)
# First document prediction
folder_list  <- folder_list_tmp[1]
Doc_predict(folder_list)
# Second document prediction
folder_list  <- folder_list_tmp[2]
Doc_predict(folder_list)
# Last document prediction
folder  <- folder_list_tmp[3]
Doc_predict(folder_list)
Doc_predict <- function(folder_list = folder_list){
# train data frame
df.tmp <- data.frame()
for (i in 1:NROW(folder_list)){
folder_path <- paste0(root_folder, "/", folder_list[i], "/")
tmp         <- get_folder_info(folder_path)
df.tmp      <- rbind(df.tmp, tmp)}
df.tmp$doc_id <- 1:NROW(df.tmp)
df.tmp        <- df.tmp %>% select(doc_id, Path, Type, Name, Extension, text)
# Create the document term matrix and Remove the doc_id column
df.tmp        <- create_dtm(df.tmp)
df.tmp        <- df.tmp[, -1]
# rearrange the column of df.tmp to equalize with dtm.all
order <- which(colnames(df.tmp) == colnames(dtm.all[,-1])[1])
for(i in 2:40){ order <- cbind(order, which(colnames(df.tmp) == colnames(dtm.all[,-1])[i])) }
# set df.tmp as test data
test <- df.tmp[, order]
d.t  <- dtm.all[, -1]
tmp  <- test
a    <- data.frame()
for (i in 1:ncol(tmp)) {if(tmp[ , i]!=0){a <- c(a, i)}}
for(j in 1:(ncol(d.t)-1)){
if (j %in% a) {d.t <- d.t[which(d.t[, j]!= 0), ]}
else          {d.t <- d.t[which(d.t[, j]== 0), ]}
}
test <- d.t[1, ]
# load the result of Stack_XGB
test_st     <- data.frame(Type        = test$Type,
tr_predict  = predict(dt_fit, newdata = test),
rf_predict  = predict(rf_fit2, newdata = test),
xgb_predict = predict(xgb_fit, newdata = test),
svm_predict = predict(tuned.svm, newdata = test),
mdl_predict = predict(mdl_fit, newdata = test),
bag_predict = predict(bag_fit, newdata = test))
result <- list(Prediction = predict(xgb_st, test_st),
Actual     = test_st$Type)
return(result)
}
####################################################################################################
# 0.Setting up environment
####################################################################################################
# remove the rdata history
rm(list=ls())
# Install and Load the packages that are needed in experiments
pkgs <- c("caret", "dplyr", "rpart", "adabag", "randomForest", "e1071", "xgboost",
"dplyr", "officer", "pdftools", "stringr", "openxlsx", "tm")
sapply(pkgs, require, character.only = TRUE)
# load the text preprocessing functions
source("DCS_function.R", encoding = "UTF-8")
# rdata that contains result
load("DCS_result.RData")
# Result Table
Acc_Table
# Show the best result : Stack_XGB
Acc_Table[which.max(Acc_Table)]
# Show the confusion matrix
Confusion_Matrix
####################################################################################################
# 2.Load the text data and predict
####################################################################################################
# Root folder path
root_folder  <- "./Folder"
# Folder list
folder_list_tmp  <- list.files(root_folder)
# First document prediction
folder_list  <- folder_list_tmp[1]
Doc_predict(folder_list)
# Second document prediction
folder_list  <- folder_list_tmp[2]
Doc_predict(folder_list)
# Last document prediction
folder_list  <- folder_list_tmp[3]
Doc_predict(folder_list)
Doc_predict <- function(folder_list = folder_list){
# train data frame
df.tmp <- data.frame()
for (i in 1:NROW(folder_list)){
folder_path <- paste0(root_folder, "/", folder_list[i], "/")
tmp         <- get_folder_info(folder_path)
df.tmp      <- rbind(df.tmp, tmp)}
df.tmp$doc_id <- 1:NROW(df.tmp)
df.tmp        <- df.tmp %>% select(doc_id, Path, Type, Name, Extension, text)
# Create the document term matrix and Remove the doc_id column
df.tmp        <- create_dtm(df.tmp)
df.tmp        <- df.tmp[, -1]
# rearrange the column of df.tmp to equalize with dtm.all
order <- which(colnames(df.tmp) == colnames(dtm.all[,-1])[1])
for(i in 2:40){ order <- cbind(order, which(colnames(df.tmp) == colnames(dtm.all[,-1])[i])) }
# set df.tmp as test data
test <- df.tmp[, order]
d.t  <- dtm.all[, -1]
tmp  <- test
a    <- data.frame()
for (i in 1:ncol(tmp)) {if(tmp[ , i]!=0){a <- c(a, i)}}
for(j in 1:(ncol(d.t)-1)){
if (j %in% a) {d.t <- d.t[which(d.t[, j]!= 0), ]}
else          {d.t <- d.t[which(d.t[, j]== 0), ]}
}
test <- d.t[1, ]
# load the result of Stack_XGB
test_st     <- data.frame(Type        = test$Type,
tr_predict  = predict(dt_fit, newdata = test),
rf_predict  = predict(rf_fit2, newdata = test),
xgb_predict = predict(xgb_fit, newdata = test),
svm_predict = predict(tuned.svm, newdata = test),
mdl_predict = predict(mdl_fit, newdata = test),
bag_predict = predict(bag_fit, newdata = test))
result <- list(Prediction = predict(xgb_st, test_st),
Actual     = test_st$Type)
return(result)
}
####################################################################################################
# 2.Load the text data and predict
####################################################################################################
# Root folder path
root_folder  <- "./Folder"
# Folder list
folder_list_tmp  <- list.files(root_folder)
# First document prediction
folder_list  <- folder_list_tmp[1]
Doc_predict(folder_list)
# Second document prediction
folder_list  <- folder_list_tmp[2]
Doc_predict(folder_list)
# Last document prediction
folder_list  <- folder_list_tmp[3]
Doc_predict(folder_list)
save.image("C:/Users/Wayne/Desktop/NIPA_TestCase/DCS_result.RData")
